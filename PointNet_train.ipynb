{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g4aidl-upc-winter-2020/3D-Shape-classification/blob/main/PointNet_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmVCqPwQbzhU"
      },
      "source": [
        "# Install all needed packages from PyG:\r\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\r\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\r\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoFs-MOrKVw8"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch_geometric.datasets import ModelNet\r\n",
        "from torch_geometric.data import DataLoader\r\n",
        "from torch_geometric.utils import to_dense_batch\r\n",
        "import torch_geometric.transforms as T\r\n",
        "from torch_geometric.transforms import SamplePoints, NormalizeScale\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import autograd\r\n",
        "\r\n",
        "import datetime\r\n",
        "from time import time\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from tensorboard import notebook\r\n",
        "%load_ext tensorboard\r\n",
        "\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iamJNZwEaoeE"
      },
      "source": [
        "##Import drive folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji4PldSCLHA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df57e6c7-74d6-4cfa-d6ed-0856ef892b46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhBEyBDMKTJU"
      },
      "source": [
        "## Set a fixed seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTbPwkkZKTR6"
      },
      "source": [
        "seed = 42\r\n",
        "\r\n",
        "#Controlling sources of randomness\r\n",
        "torch.manual_seed(seed)  #Sets the seed for generating random numbers for all devices (both CPU and CUDA)\r\n",
        "\r\n",
        "#Random number generators in other libraries\r\n",
        "np.random.seed(seed)\r\n",
        "\r\n",
        "#CUDA convolution benchmarking\r\n",
        "torch.backends.cudnn.benchmark = False #ensures that CUDA selects the same algorithm each time an application is run\r\n",
        "\r\n",
        "#Avoiding nondeterministic algorithms\r\n",
        "torch.use_deterministic_algorithms(True) #use “deterministic” algorithms (given the same input always produce the same output)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wI5yPp_NiO"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9EOF7z_Nt3"
      },
      "source": [
        "learning_rate = 0.001\r\n",
        "train_batch_size = 32\r\n",
        "val_batch_size = 32\r\n",
        "num_epochs = 20\r\n",
        "weight_decay = 1e-3 "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwQLQDA3WK3e"
      },
      "source": [
        "## Instantiate Tensorboard Writer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_jU3MGnP5N"
      },
      "source": [
        "### Create log folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhJSdm5TvNaV"
      },
      "source": [
        "root='/content/drive/MyDrive/Proyecto/Colabs/experiments/logs/PointNet'\r\n",
        "train_logdir = os.path.join(root, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'train')\r\n",
        "val_logdir = os.path.join(root, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'validation')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmLLGeZinV65"
      },
      "source": [
        "### Create summary writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvB8h1khnUxc"
      },
      "source": [
        "train_writer = SummaryWriter(log_dir=train_logdir)\r\n",
        "val_writer = SummaryWriter(log_dir=val_logdir)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr9LupPTbvsU"
      },
      "source": [
        "# PointNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6C-wwDWb7iV"
      },
      "source": [
        "This is an implementation of [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) using PyTorch *Geometric*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ectzRUBgcewc"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7S0n1sucFeF"
      },
      "source": [
        "# Import ModelNet10 dataset from PyG\r\n",
        "dataset = ModelNet(root='/content/drive/MyDrive/Proyecto/Colabs/ModelNet', name= \"10\", train=True, pre_transform=T.SamplePoints(num=1024)) #train dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1gIri-UHm7P"
      },
      "source": [
        "#We load from two text files the indices of train and validation data \r\n",
        "train = open(\"/content/drive/MyDrive/Proyecto/Colabs/train_split.txt\", 'r')\r\n",
        "train_idx = []\r\n",
        "for idx in train:\r\n",
        "  train_idx.append(int(idx))\r\n",
        "\r\n",
        "val = open(\"/content/drive/MyDrive/Proyecto/Colabs/val_split.txt\", 'r')\r\n",
        "val_idx = []\r\n",
        "for idx in val:\r\n",
        "  val_idx.append(int(idx))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzmTHcUzAV4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e84cc2d-24d7-43e5-eca3-f6a719152885"
      },
      "source": [
        "train_dataset = dataset[train_idx]\r\n",
        "val_dataset = dataset[val_idx]\r\n",
        "\r\n",
        "print('Datasets info:')\r\n",
        "print('--------------')\r\n",
        "print('Train dataset size: ', len(train_dataset))\r\n",
        "print('Validation dataset size: ', len(val_dataset))\r\n",
        "print('Number of classes: ', dataset.num_classes) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets info:\n",
            "--------------\n",
            "Train dataset size:  3193\n",
            "Validation dataset size:  798\n",
            "Number of classes:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWmHm9ybTAzf"
      },
      "source": [
        "### Normalize Input points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iDPljFTTA72"
      },
      "source": [
        "#Centers and normalizes node positions to the interval (-1,1) \r\n",
        "train_dataset.transform = NormalizeScale()\r\n",
        "val_dataset.transform = NormalizeScale()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0lHvGwYCug3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVADHsQOLE5R"
      },
      "source": [
        "### Make sure your runtime has a GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoO0XuudLFOT"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "assert not device.type == 'cpu', \"Change Runtime Type -> GPU\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxAIsENchj7O"
      },
      "source": [
        "### Loading the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MolYIQ4dmWvy"
      },
      "source": [
        "#We include a new file path that will point to modules that we want to import\r\n",
        "sys.path.append('/content/drive/MyDrive/Proyecto/Colabs/architectures/PointNet')  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNhtiqggDJJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3db3b2e-ffc3-4dc0-8dc2-98733123d49d"
      },
      "source": [
        "#Import the model from a python script\r\n",
        "from PointNet_Architecture import ClassificationPointNet\r\n",
        "model = ClassificationPointNet()  #Instantiate the model\r\n",
        "model.to(device)    # Pass the model to GPU(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificationPointNet(\n",
              "  (transform): Transform(\n",
              "    (input_transform): Tnet(\n",
              "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (feature_transform): Tnet(\n",
              "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
              "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
              "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
              "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNhZxuaQhWzy"
      },
      "source": [
        "#### Pararameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTTzDt0mnNhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76bfdc4-d40b-464e-863d-2fc7cf32acac"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 3463763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7jXu1f9gas7"
      },
      "source": [
        "### Accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bljN_FRyfJ2E"
      },
      "source": [
        "def accuracy(output, target):\r\n",
        "  pred = output.argmax(dim=1)  # get the index of the max log-probability\r\n",
        "  return (pred == target).sum().item() / target.numel()  #return the mean accuracy in the batch"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1C8ESuhfgCF"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1gITlsMf_hM"
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, epoch, scheduler):   #Training function for one epoch\r\n",
        "  \r\n",
        "  model.train() #Activate the train=True flag inside the model\r\n",
        "  losses = []\r\n",
        "  accs = []\r\n",
        "\r\n",
        "  lr = scheduler.get_last_lr()    \r\n",
        "\r\n",
        "  for batch_idx, data in enumerate(train_loader, 1):\r\n",
        "\r\n",
        "      inputs = to_dense_batch(data.pos, batch=data.batch)  #return a tuple where in the first position there are the points of every pointcloud in the batch\r\n",
        "      \r\n",
        "      optimizer.zero_grad()  #setting all the gradient to zero\r\n",
        "\r\n",
        "      output = model(inputs[0].to(device).float().transpose(1,2))  #Pass the inputs through the network and compute the outputs\r\n",
        "      \r\n",
        "      loss = criterion(output.to(device), data.y.to(device))  #Calculate the loss in the batch\r\n",
        "      \r\n",
        "      loss.backward() #Backprop over the loss function through the network\r\n",
        "      \r\n",
        "      acc = 100 * accuracy(output.to(device), data.y.to(device))  #Calculate the mean accuracy in the batch\r\n",
        "\r\n",
        "      losses.append(loss.item()) #save the loss value in a list of losses\r\n",
        "      \r\n",
        "      accs.append(acc) #save the accuracy value in a list of accuracies\r\n",
        "\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      if batch_idx >= len(train_loader):\r\n",
        "          print('Train Epoch: {} \\tLR: {} \\tAverage Loss: {:.4f}\\tAverage Acc: {:.2f} %'.format(\r\n",
        "              epoch, lr , np.mean(losses), np.mean(accs)))\r\n",
        "          \r\n",
        "          train_writer.add_scalar('Loss', np.mean(losses), epoch) #log training loss for one epoch to Tensorboard\r\n",
        "          train_writer.add_scalar('Acc', np.mean(accs), epoch)    #log training accuracy for one epoch to Tensorboard\r\n",
        "\r\n",
        "  return np.mean(losses), np.mean(accs)\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "misdA5uRfuBS"
      },
      "source": [
        "### Validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29opQyJMfsgD"
      },
      "source": [
        "def eval_epoch(model, val_loader, criterion, epoch):  #evaluation function after one epoch of training\r\n",
        "  \r\n",
        "  model.eval() #Activate the train=False flag inside the model\r\n",
        "  eval_losses = []\r\n",
        "  eval_accs = []\r\n",
        "  with torch.no_grad():\r\n",
        "    for data in val_loader:\r\n",
        "      inputs = to_dense_batch(data.pos, batch=data.batch)  #return a tuple where in the first position there are the points of every pointcloud in the batch\r\n",
        "      \r\n",
        "      output = model(inputs[0].to(device).float().transpose(1,2))  #Pass the inputs through the network \r\n",
        "      \r\n",
        "      eval_loss = criterion(output.to(device), data.y.to(device))\r\n",
        "      eval_losses.append(eval_loss.item()) #save the loss value in a list of losses\r\n",
        "      \r\n",
        "      eval_acc = 100 * accuracy(output.to(device), data.y.to(device)) #Calculate the accuracy in the batch\r\n",
        "      eval_accs.append(eval_acc) #save the accuracy value in a list of accuracies\r\n",
        "    \r\n",
        "    print('Val Epoch: {} \\tAverage loss: {:.4f}\\tAverage Acc: {:.2f} %'.format(\r\n",
        "        epoch, np.mean(eval_losses), np.mean(eval_accs)))\r\n",
        "    \r\n",
        "    val_writer.add_scalar('Loss', np.mean(eval_losses), epoch)  #log validation loss for one epoch to Tensorboard\r\n",
        "    val_writer.add_scalar('Acc',  np.mean(eval_accs), epoch)    #log validation accuracy for one epoch to Tensorboard\r\n",
        "\r\n",
        "  return np.mean(eval_losses), np.mean(eval_accs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbaAzbN1bpHd"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8YtqmphC_v-"
      },
      "source": [
        "Let's pass the datasets through the DataLoader in order to obtain batches of samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhMOsGbtC8GR"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)  #train set\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)       #val set"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBIcXrLLezaP"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbbSL_9DSn6"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate, weight_decay = weight_decay)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERoqa01amF7z"
      },
      "source": [
        "## Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R0tkMRxmIs4"
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5) #Learning rate is divided by 2 every 20 epochs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAtvrbyLe5-3"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdWadi30e6gk"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()  #useful to train a classification problem with N classes (logsoftmax layer + NLLLoss)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJDe-7gJj2xK"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C0MtcpVgG4i"
      },
      "source": [
        "def train_net(model, train_loader, val_loader, optimizer, criterion, num_epochs, scheduler): \r\n",
        "  \"\"\" Function that trains and evals a network for n epochs.\r\n",
        "  \"\"\"\r\n",
        "  #with autograd.detect_anomaly():\r\n",
        "  best_accuracy = 0.0\r\n",
        "  for epoch in range(1, num_epochs + 1):\r\n",
        "    tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, criterion, epoch, scheduler)  #train the model\r\n",
        "    val_loss, val_acc = eval_epoch(model, val_loader, criterion, epoch)              #eval the model \r\n",
        "    \r\n",
        "    scheduler.step() #Step for LR decay\r\n",
        "    \r\n",
        "    if best_accuracy < val_acc:\r\n",
        "      best_accuracy = val_acc\r\n",
        "      torch.save(model.state_dict(), train_logdir + '/best_params.pt')  #save the model state for best val accuracy\r\n",
        "  \r\n",
        "  return best_accuracy  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIJmfYuDRFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1740a5-5adc-4116-c94e-a97eae84df9f"
      },
      "source": [
        "best_accuracy = train_net(model, train_loader, val_loader, optimizer, criterion, num_epochs, scheduler)\r\n",
        "print('Best validation accuracy = ', best_accuracy)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 \tLR: [0.001] \tAverage Loss: 0.9617\tAverage Acc: 69.99 %\n",
            "Val Epoch: 1 \tAverage loss: 0.9506\tAverage Acc: 71.04 %\n",
            "Train Epoch: 2 \tLR: [0.001] \tAverage Loss: 0.5532\tAverage Acc: 81.45 %\n",
            "Val Epoch: 2 \tAverage loss: 0.4611\tAverage Acc: 85.72 %\n",
            "Train Epoch: 3 \tLR: [0.001] \tAverage Loss: 0.4187\tAverage Acc: 85.89 %\n",
            "Val Epoch: 3 \tAverage loss: 0.5195\tAverage Acc: 83.48 %\n",
            "Train Epoch: 4 \tLR: [0.001] \tAverage Loss: 0.3514\tAverage Acc: 87.71 %\n",
            "Val Epoch: 4 \tAverage loss: 1.2383\tAverage Acc: 61.89 %\n",
            "Train Epoch: 5 \tLR: [0.001] \tAverage Loss: 0.3533\tAverage Acc: 87.83 %\n",
            "Val Epoch: 5 \tAverage loss: 0.9157\tAverage Acc: 72.03 %\n",
            "Train Epoch: 6 \tLR: [0.001] \tAverage Loss: 0.3018\tAverage Acc: 89.59 %\n",
            "Val Epoch: 6 \tAverage loss: 0.5249\tAverage Acc: 79.82 %\n",
            "Train Epoch: 7 \tLR: [0.001] \tAverage Loss: 0.2842\tAverage Acc: 90.13 %\n",
            "Val Epoch: 7 \tAverage loss: 0.3470\tAverage Acc: 88.50 %\n",
            "Train Epoch: 8 \tLR: [0.001] \tAverage Loss: 0.2737\tAverage Acc: 91.05 %\n",
            "Val Epoch: 8 \tAverage loss: 0.4214\tAverage Acc: 84.83 %\n",
            "Train Epoch: 9 \tLR: [0.001] \tAverage Loss: 0.3099\tAverage Acc: 90.14 %\n",
            "Val Epoch: 9 \tAverage loss: 0.5618\tAverage Acc: 83.96 %\n",
            "Train Epoch: 10 \tLR: [0.001] \tAverage Loss: 0.2695\tAverage Acc: 91.81 %\n",
            "Val Epoch: 10 \tAverage loss: 0.3739\tAverage Acc: 87.97 %\n",
            "Train Epoch: 11 \tLR: [0.001] \tAverage Loss: 0.2310\tAverage Acc: 92.71 %\n",
            "Val Epoch: 11 \tAverage loss: 0.2579\tAverage Acc: 92.36 %\n",
            "Train Epoch: 12 \tLR: [0.001] \tAverage Loss: 0.2425\tAverage Acc: 92.01 %\n",
            "Val Epoch: 12 \tAverage loss: 0.5624\tAverage Acc: 83.09 %\n",
            "Train Epoch: 13 \tLR: [0.001] \tAverage Loss: 0.2368\tAverage Acc: 92.05 %\n",
            "Val Epoch: 13 \tAverage loss: 0.4278\tAverage Acc: 84.10 %\n",
            "Train Epoch: 14 \tLR: [0.001] \tAverage Loss: 0.2369\tAverage Acc: 92.30 %\n",
            "Val Epoch: 14 \tAverage loss: 0.3601\tAverage Acc: 89.45 %\n",
            "Train Epoch: 15 \tLR: [0.001] \tAverage Loss: 0.2739\tAverage Acc: 91.40 %\n",
            "Val Epoch: 15 \tAverage loss: 0.2611\tAverage Acc: 92.12 %\n",
            "Train Epoch: 16 \tLR: [0.001] \tAverage Loss: 0.2319\tAverage Acc: 92.08 %\n",
            "Val Epoch: 16 \tAverage loss: 0.9245\tAverage Acc: 71.70 %\n",
            "Train Epoch: 17 \tLR: [0.001] \tAverage Loss: 0.2443\tAverage Acc: 91.65 %\n",
            "Val Epoch: 17 \tAverage loss: 0.7910\tAverage Acc: 79.32 %\n",
            "Train Epoch: 18 \tLR: [0.001] \tAverage Loss: 0.2150\tAverage Acc: 92.55 %\n",
            "Val Epoch: 18 \tAverage loss: 0.4822\tAverage Acc: 81.95 %\n",
            "Train Epoch: 19 \tLR: [0.001] \tAverage Loss: 0.2156\tAverage Acc: 93.23 %\n",
            "Val Epoch: 19 \tAverage loss: 0.3453\tAverage Acc: 87.23 %\n",
            "Train Epoch: 20 \tLR: [0.001] \tAverage Loss: 0.1988\tAverage Acc: 93.32 %\n",
            "Val Epoch: 20 \tAverage loss: 0.2135\tAverage Acc: 92.99 %\n",
            "Best validation accuracy =  92.99166666666666\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}